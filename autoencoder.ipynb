{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anomaly Dection on ECG Heartbeat Categorization Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "In this Jupyter notebook, we will embark on an exploration of the intriguing field of electrocardiogram (ECG) signal processing and classification, utilizing the capabilities of a Convolutional Neural Network (CNN) Autoencoder. Our chosen dataset for this journey is the PTB Diagnostic ECG Database, a meticulously curated collection of ECG signals crafted explicitly for diagnostic purposes. Our primary objective is to construct a robust Autoencoder model tailored for the task of detecting anomalous electrocardiogram (ECG) signals.\n",
        "\n",
        "An electrocardiogram (ECG) is a medical test that detects cardiac (heart) abnormalities by measuring the electrical activity generated by the heart as it contracts.\n",
        "ECGs are used to investigate and diagnose conditions affecting the heart. They can help to detect arrhythmias, heart attacks, and other heart conditions.\n",
        "\n",
        "\n",
        "![ecg_example](img/SinusRhythmLabels.jpg)\n",
        "\n",
        "There are three main components to an ECG:\n",
        "\n",
        "- The P wave, which represents depolarization of the atria.\n",
        "- The QRS complex, which represents depolarization of the ventricles.\n",
        "- The T wave, which represents repolarization of the ventricles.\n",
        "\n",
        "\n",
        "Source: [Wikipedia](https://en.wikipedia.org/wiki/Electrocardiography)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The [PTB Diagnostic ECG](https://doi.org/10.13026/C28C71) Database is a collection of 14,552 ECG recordings sourced from Physionet's PTB Diagnostic Database. These ECG signals are categorized into two classes: normal heartbeats and those affected by cardiac abnormalities. The dataset is sampled at 125Hz, providing high-resolution data for in-depth analysis.\n",
        "\n",
        "Let's delve into some essential details about the PTB Diagnostic ECG Database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-0covd3HiWn",
        "outputId": "26df0827-c37d-4a75-9e33-b56f45aac6fa"
      },
      "outputs": [],
      "source": [
        "normal_df = pd.read_pickle(\"./data/kaggleECG/ptbdb_normal.pkl\")\n",
        "anomaly_df = pd.read_pickle(\"./data/kaggleECG/ptbdb_abnormal.pkl\")\n",
        "print(f'Number of normal examples for {normal_df.shape[0]}')\n",
        "print(f'Number of anomalous examples for {anomaly_df.shape[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot a couple of examples of normal and anoumalous ECGs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_examples(nlines = 3):\n",
        "    fig, axs = plt.subplots(1,2,figsize = (8,3), dpi = 160)\n",
        "    axs[0].plot(normal_df.values[:nlines].T)\n",
        "    axs[1].plot(anomaly_df.values[0:nlines].T)\n",
        "    axs[0].set_title('Normal ECGs')\n",
        "    axs[1].set_title('Anoumalous ECGs')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_examples()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some clear similarity is visible in the normal ECGs, that are not present in the anomalous ECGs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFxPbGqpHiWr",
        "outputId": "4b3b3776-02e7-466d-9f52-648864e76062"
      },
      "source": [
        "## AutoEncoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Autoencoders are a class of neural network architectures commonly used in unsupervised machine learning and deep learning tasks. Their primary purpose is to discover and learn efficient representations of data by encoding it into a lower-dimensional latent space and subsequently decoding it back to its original form. Autoencoders play a crucial role in various applications, such as dimensionality reduction, data denoising, anomaly detection, and generative modeling.\n",
        "\n",
        "The core components of an autoencoder consist of an encoder and a decoder. The encoder maps input data to the latent space, while the decoder reconstructs the data from its encoded representation. During training, autoencoders aim to minimize the reconstruction error between the input and the decoded output, which results in the learning of meaningful data representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![autoencoder](img/autoencoder.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " For the encored, we will use some 1D concolutional layers on the input data.\n",
        "\n",
        "<img src=\"img/conv1d.png\" alt=\"drawing\" width=\"1000\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krs_9edoHiWr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class ConvEncoder(nn.Module):\n",
        "    \"\"\"ConvEncoder.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        output_size: int = 64\n",
        "    ):\n",
        "        \"\"\"__init__.\n",
        "\n",
        "        :param channels:\n",
        "        :type channels: int\n",
        "        :param hidden:\n",
        "        :type hidden: int\n",
        "        :param bottleneck:\n",
        "        :type bottleneck: int\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1d_1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=6, stride=3, padding=0)\n",
        "        self.conv1d_2 = nn.Conv1d(in_channels=64,out_channels=32,kernel_size=6, stride=3)\n",
        "        self.conv1d_3 = nn.Conv1d(in_channels=32,out_channels=1,kernel_size=15)\n",
        "        self.lrelu = nn.LeakyReLU()\n",
        "        self.fc1 = nn.Linear(5, output_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"forward.\n",
        "\n",
        "        :param x:\n",
        "        :type x: torch.Tensor\n",
        "        \"\"\"\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.conv1d_1(x)\n",
        "        x = self.lrelu(x)\n",
        "        x = self.conv1d_2(x)\n",
        "        x = self.lrelu(x)\n",
        "        x = self.conv1d_3(x)\n",
        "        x=x.squeeze(1)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the decoder part, we will use transpose convolution.\n",
        "\n",
        "<img src=\"img/transconv.png\" alt=\"drawing\" width=\"1000\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class ConvDecoder(nn.Module):\n",
        "    \"\"\"ConvDecoder.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        output_size: int,\n",
        "        input_size: int = 64\n",
        "    ):\n",
        "        \"\"\"__init__.\n",
        "\n",
        "        :param channels:\n",
        "        :param hidden:\n",
        "        :param bottleneck:\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc2 = nn.Linear(input_size, 5)\n",
        "        self.deconv1d_1 = nn.ConvTranspose1d(in_channels=1, out_channels=32,kernel_size=15)\n",
        "        self.deconv1d_2 = nn.ConvTranspose1d(in_channels=32, out_channels=64,kernel_size=6, stride=3)\n",
        "        self.deconv1d_3 = nn.ConvTranspose1d(in_channels=64, out_channels=1, kernel_size=6, stride=3, padding=0)\n",
        "        self.linear = nn.Linear(183, output_size)  # Output dimension matches input_dim\n",
        "        self.lrelu = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"forward.\n",
        "\n",
        "        :param x:\n",
        "        :type x: torch.Tensor\n",
        "        \"\"\"\n",
        "        x = self.fc2(x)\n",
        "        x = torch.unsqueeze(x,1)\n",
        "        x = self.deconv1d_1(x)\n",
        "        x = self.lrelu(x)\n",
        "        x = self.deconv1d_2(x)\n",
        "        x = self.lrelu(x)\n",
        "        x = self.deconv1d_3(x)\n",
        "        x = x.squeeze(1)\n",
        "        x= self.linear(x)\n",
        "\n",
        "\n",
        "        return x\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ConvAutoencoder will be:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    \"\"\"ConvAutoencoder.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "        input_size: int,\n",
        "        latent_size: int = 64,\n",
        "        device: str = 'cpu'\n",
        "    ):\n",
        "        \"\"\"__init__.\n",
        "\n",
        "        :param channels:\n",
        "        :param hidden:\n",
        "        :param bottleneck:\n",
        "        :param device:\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = ConvEncoder(\n",
        "            input_size = input_size,\n",
        "            output_size = latent_size).to(device)\n",
        "        self.decoder = ConvDecoder(\n",
        "            input_size = latent_size,\n",
        "            output_size = input_size).to(device)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"forward.\n",
        "\n",
        "        :param x:\n",
        "        :type x: torch.Tensor\n",
        "        \"\"\"\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take the normal dataset and split it in train/test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test = train_test_split(normal_df.values, test_size=0.15, random_state=45, shuffle=True)\n",
        "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}, anomaly shape: {anomaly_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5USCeHBYHiWu",
        "outputId": "1f22063c-4e07-4269-e203-d92da02e49c8"
      },
      "outputs": [],
      "source": [
        "BATCHSIZE = 32\n",
        "# Instantiate the PyTorch model\n",
        "input_dim = X_train.shape[-1]\n",
        "latent_dim = 64\n",
        "model = ConvAutoencoder(input_dim, latent_dim)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "# Convert your dataset to PyTorch DataLoader\n",
        "# Assuming your dataset is in the form of a PyTorch Tensor\n",
        "train_loader = DataLoader(TensorDataset(torch.tensor(X_train)), batch_size=BATCHSIZE, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(torch.tensor(X_test)), batch_size=BATCHSIZE, shuffle=True)\n",
        "\n",
        "# Lists to store training and testing losses\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# Training the Autoencoder\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs = data[0].float()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, inputs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Testing\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_loader):\n",
        "            inputs = data[0].float()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, inputs)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    # Calculate average losses\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "\n",
        "    # Store losses in arrays\n",
        "    train_losses.append(avg_train_loss)\n",
        "    test_losses.append(avg_test_loss)\n",
        "\n",
        "    # Print training and testing statistics\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize the train and test loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (4,3), dpi = 160)\n",
        "plt.plot(train_losses, label=\"Training loss\")\n",
        "plt.plot(test_losses, label=\"Test loss\", ls=\"--\")\n",
        "plt.legend(shadow=True, frameon=True, facecolor=\"inherit\", loc=\"best\", fontsize=9)\n",
        "plt.title(\"Training loss\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now evaluate the model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z910ryRpHiWu"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data):\n",
        "    model.eval()\n",
        "    inputs = torch.tensor(data).float()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "    return outputs.detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBc-5cCWHiWv"
      },
      "outputs": [],
      "source": [
        "outputs_tr = evaluate_model(model,X_train)\n",
        "outputs_ts = evaluate_model(model,X_test)\n",
        "outputs_anomaly = evaluate_model(model,anomaly_df.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize if the autoencoder is not correctly reconstructing anomalies as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_prediction(data, predictions, title, ax):\n",
        "  loss = np.mean(abs(data - predictions))\n",
        "\n",
        "  ax.plot(data, label='True')\n",
        "  ax.plot(predictions, label='Reconstructed')\n",
        "  ax.set_title(f'{title} (MAE: {np.around(loss, 2)})')\n",
        "  ax.legend()\n",
        "  \n",
        "def plot_predictions(ncols = 4):\n",
        "  fig, axs = plt.subplots(\n",
        "    nrows=2,\n",
        "    ncols=ncols,\n",
        "    sharey=True,\n",
        "    sharex=True,\n",
        "    figsize=(3*ncols, 6)\n",
        "  )\n",
        "\n",
        "  for i, (data, prediction) in enumerate(zip(X_test[:ncols], outputs_ts[:ncols])):\n",
        "    plot_prediction(data, prediction, title='Normal', ax=axs[0, i])\n",
        "\n",
        "  for i,(data, prediction) in enumerate(zip(anomaly_df.values[:ncols], outputs_anomaly[:ncols])):\n",
        "    plot_prediction(data, prediction, title='Anomaly', ax=axs[1, i])\n",
        "\n",
        "  fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_predictions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's define our reconstruction error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYEXblyiHiWw"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpag2h5-HiWw"
      },
      "outputs": [],
      "source": [
        "error_tr = np.mean(abs(outputs_tr - X_train), axis = 1)\n",
        "error_ts = np.mean(abs(outputs_ts - X_test), axis = 1)\n",
        "error_anomaly = np.mean(abs(outputs_anomaly - anomaly_df.values), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRme9v5HHiWw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdV7-mfeHiWw",
        "outputId": "d27e01c4-c13c-49f3-b080-330730ac487d"
      },
      "outputs": [],
      "source": [
        "bins = np.linspace(0,0.15,100)\n",
        "fig = plt.figure(figsize=(4,3), dpi = 160)\n",
        "plt.hist(error_tr, bins = bins, label = 'Train', density = True, histtype='step')\n",
        "plt.hist(error_ts, bins = bins, label = 'Test', density = True, histtype='step')\n",
        "plt.hist(error_anomaly, bins = bins, label = 'Anomaly', density = True, histtype='step')\n",
        "plt.legend()\n",
        "plt.xlabel('Reconstruction error')\n",
        "plt.ylabel('a.u.')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the Reconstruction error we can now fix threshold to distinguish between normal and anomalous data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o3hY4GPHiWx"
      },
      "outputs": [],
      "source": [
        "THR = 0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Excercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Excercise 1: produce the ROC curve plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def true_positive(anomaly_score, thr):\n",
        "    return(len(anomaly_score[anomaly_score > thr])/len(anomaly_score))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TP = true_positive(error_anomaly, THR)\n",
        "print(f'True Positive Rate is {TP:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define here the FP function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define here the plot function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Excercise 2: train a linear autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define here the LinearAutoencoder, replacing the Conv1d Laters with lieanr layers (https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dnn_reco",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
